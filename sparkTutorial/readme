Refer udemy course for this spark tutorial.
Documentation :- https://spark.apache.org/docs/latest/rdd-programming-guide.html
                https://spark.apache.org/docs/latest/

1. java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries
    -   Download winutils.exe and keep in folder. Add system property "hadoop.home.dir" to folder containing bin.
        eg: System.setProperty("hadoop.home.dir","C:\\hadoop"); where hadoop folder contains bin folder and bin contains winutils.exe
        Refer example :- org.example.SparkTextFileRead

2. Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProviderTokenIssuer -
    -   Added following dependency
        <dependency>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-common</artifactId>
                    <version>2.10.2</version>
                </dependency>

3. Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
    -  Download winutils.exe and keep in folder. Add system property "hadoop.home.dir" to folder containing bin.
       Refer example :- org.example.SparkTextFileRead

4. When you use foreach method of RDD then we might get incorrect result in case of sorting. We might not get sorted result. It becuase
    foreach on RDD works different than forEach of List. When we do foreach on RDD driver program sends this function to worker nodes. Then worker nodes
    runs this functions parallally on different partitions. Thats why we see unexpected outcome.
    If in case of sort we want to perform forEach then we should do on final result List instead of on RDD.