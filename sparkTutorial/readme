Refer udemy course for this spark tutorial.

Documentation :- https://spark.apache.org/docs/latest/rdd-programming-guide.html
                https://spark.apache.org/docs/latest/

Spark RDD section -
            Resilient Distributed Data. Resilient means if any node gets failed even then these data gets recalculated. Distributed means it can be distributed on multiple nodes

            For Spark RDD we don't need spark-sql dependency.

            Examples present in : src/main/java/org/rdd/example/*

1. java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries
    -   Download winutils.exe and keep in folder. Add system property "hadoop.home.dir" to folder containing bin.
        eg: System.setProperty("hadoop.home.dir","C:\\hadoop"); where hadoop folder contains bin folder and bin contains winutils.exe
        Refer example :- SparkTextFileRead

2. Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProviderTokenIssuer -
    -   Added following dependency
        <dependency>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-common</artifactId>
                    <version>2.10.2</version>
                </dependency>

3. Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
    -  Download winutils.exe and keep in folder. Add system property "hadoop.home.dir" to folder containing bin.
       Refer example :- SparkTextFileRead

4. When you use foreach method of RDD then we might get incorrect result in case of sorting. We might not get sorted result. It becuase
    foreach on RDD works different than forEach of List. When we do foreach on RDD driver program sends this function to worker nodes. Then worker nodes
    runs this functions parallally on different partitions. That's why we see unexpected outcome.
    If in case of sort we want to perform forEach then we should do on final result List instead of on RDD.

5. Transformation vs action :- Transformations are just execution plan given to spark. Actions where spark starts operation.
    E.g. transformation -> rdd.map(), rdd.filter(). rdd.flatmap().  actions -> rdd.collect(), rdd.take().

    Narrow transformation :- Narrow transformations are those transformations where data don't get shuffled between partitions.
    E.g rdd.filter() . In this filter operation each rdd can work independently without needing data of another partition.

    Wide transformation :- Wide transformations are those transformations where data gets shuffled between partitions.
    E.g. rdd.groupByKey(); In this group by key operation data gets shuffled between rdd's so that one key data will gather in one partition.

6. Stage in spark :- Stage is series of transformations that don't need shuffle. When there is a point where shuffle is needed spark creates new stage.
       So we can say that when stage reach wide transformation it creates new stage.
       Name of stage is the last operation name of that stage.

7. Partitions in spark :- Partition in spark is a atomic block of data which resides at a single node at any given time. Partitions can run in parallel as no 2 partitions
       are dependent at any given time. By default spark creates each partition with 32mb of data. If file size is 90mb then spark will create 3 partitions with size32mb,
       32mb and 26mb.
       If thread pool size 32 and we have 64 partitions then at a time 32 partitions can be run.

8. Key skew :- Its situation where data gets populated in one partition and other partitions stays empty.
       To solve this use salting technique. Read more when you actually face this problem.

9. Job in spark :- Job in spark gets created when we call any action. Means if we call 2 actions then 2 jobs will get created. 2nd action will also start from last read or write
        to memory. If no read write has happened between start to 1st action then 2nd action will starts its operation from start of the program i.e. start of the spark execution
        plan. So its better to cache or persist the intermediate results so that new job will not start whole operation again. Use DAG to see which operations gets computed
        repeatedly and cache result of that operation to avoid such repeated computing.
        Example : SparkJobExample

10. RDD :- Resilient Distributed Data. Resilient means if any node gets failed even then these data gets recalculated. Distributed means it can be distributed on multiple
        nodes. RDD are immutable

11. Group by :- Group by is used to group data based on key. Group by works in 2 stages i.e. shuffle data i.e write and read and map it to based on key .
--------------------------

Spark sql section -
            Spark sql is newer version of spark  which comes with rich set of API. It works on Dataset not on RDD but dataset is internally works on RDD only.
            Need spark-sql dependency along with spark core and other.

            Example present in : src/main/java/org/sql/example/*

            Spark Sql Dataset supports all operations supported by RDD like filter, groupBy, map ....etc

            Dataset are immutable like RDD on every step it returns new RDD or Dataset doesn't update existing one.

1. function class : Spark sql provides function class which contains static methods which we can use in program. We can use static import to import this class so
        that we can use static methods directly without using class name.

        Example : org.sql.example.SparkSQLFunctionClassExample
